{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/media-sequence/media_sequence.csv\n",
      "/kaggle/input/data-science-bowl-2019/train.csv\n",
      "/kaggle/input/data-science-bowl-2019/train_labels.csv\n",
      "/kaggle/input/data-science-bowl-2019/specs.csv\n",
      "/kaggle/input/data-science-bowl-2019/test.csv\n",
      "/kaggle/input/data-science-bowl-2019/sample_submission.csv\n",
      "/kaggle/input/dsb-media-sequence/media_sequence.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 500/5seeds/1200+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit,StratifiedKFold,TimeSeriesSplit,KFold,GroupKFold,train_test_split,GroupShuffleSplit,StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score,mean_squared_error,mean_absolute_error,log_loss,confusion_matrix\n",
    "import sqlite3\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "import gc\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from bayes_opt import BayesianOptimization\n",
    "import re\n",
    "from string import punctuation\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial import Delaunay\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_accuracy_group(accuracy):\n",
    "    if accuracy ==0:\n",
    "        return 0\n",
    "    elif accuracy ==1:\n",
    "        return 3\n",
    "    elif accuracy ==0.5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "@jit\n",
    "def qwk(a1, a2):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n",
    "\n",
    "    :param a1:\n",
    "    :param a2:\n",
    "    :param max_rat:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    e = e / a1.shape[0]\n",
    "\n",
    "    return 1 - o / e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kappa(y_pred,y_true,coefficients):\n",
    "    preds = y_pred.copy()\n",
    "    preds[y_pred <= coefficients[0]] = 0\n",
    "    preds[np.where(np.logical_and(y_pred > coefficients[0], y_pred <= coefficients[1]))] = 1\n",
    "    preds[np.where(np.logical_and(y_pred > coefficients[1], y_pred <= coefficients[2]))] = 2\n",
    "    preds[y_pred > coefficients[2]] = 3\n",
    "    return qwk(y_true,preds)\n",
    "\n",
    "def get_pred(y_pred,coefficients):\n",
    "    preds = y_pred.copy()\n",
    "    preds[y_pred <= coefficients[0]] = 0\n",
    "    preds[np.where(np.logical_and(y_pred > coefficients[0], y_pred <= coefficients[1]))] = 1\n",
    "    preds[np.where(np.logical_and(y_pred > coefficients[1], y_pred <= coefficients[2]))] = 2\n",
    "    preds[y_pred > coefficients[2]] = 3\n",
    "    return preds\n",
    "\n",
    "def opt_more_coeff(oof_predict,y_train,coef):\n",
    "    score1 = 0\n",
    "    k = 0\n",
    "    for a in np.arange(-0.1,0.1,0.01):\n",
    "        for b in np.arange(-0.1,0.1,0.01):\n",
    "            for c in np.arange(-0.1,0.1,0.01):\n",
    "                coefficients = [coef[0]+a, coef[1]+b, coef[2]+c]\n",
    "                preds = oof_predict.copy()\n",
    "                preds[oof_predict <= coefficients[0]] = 0\n",
    "                preds[np.where(np.logical_and(oof_predict > coefficients[0], oof_predict <= coefficients[1]))] = 1\n",
    "                preds[np.where(np.logical_and(oof_predict > coefficients[1], oof_predict <= coefficients[2]))] = 2\n",
    "                preds[oof_predict > coefficients[2]] = 3\n",
    "                kappa_1 = qwk(y_train, preds)\n",
    "                k+=1\n",
    "                if kappa_1>score1:\n",
    "                    temp_save = [coef[0]+a, coef[1]+b, coef[2]+c]\n",
    "                    #print(np.round(kappa_1,8),'Per:',str(round((k/64000)*100,2))+'%',k,temp_save)\n",
    "                    score1 = kappa_1\n",
    "    coef = temp_save.copy()\n",
    "    ### second \n",
    "    k=0\n",
    "    for a in np.arange(-0.04,0.04,0.004):\n",
    "        for b in np.arange(-0.04,0.04,0.004):\n",
    "            for c in np.arange(-0.04,0.04,0.004):\n",
    "                coefficients = [coef[0]+a, coef[1]+b, coef[2]+c]\n",
    "                preds = oof_predict.copy()\n",
    "                preds[oof_predict <= coefficients[0]] = 0\n",
    "                preds[np.where(np.logical_and(oof_predict > coefficients[0], oof_predict <= coefficients[1]))] = 1\n",
    "                preds[np.where(np.logical_and(oof_predict > coefficients[1], oof_predict <= coefficients[2]))] = 2\n",
    "                preds[oof_predict > coefficients[2]] = 3\n",
    "                kappa_1 = qwk(y_train, preds)\n",
    "                k+=1\n",
    "                if kappa_1>score1:\n",
    "                    temp_save = [coef[0]+a, coef[1]+b, coef[2]+c]\n",
    "                    #print(np.round(kappa_1,8),'Per:',str(round((k/64000)*100,2))+'%',k,temp_save)\n",
    "                    score1 = kappa_1\n",
    "    return temp_save,score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n",
    "test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "train['timestamp'] = pd.to_datetime(train.timestamp)\n",
    "test['timestamp'] = pd.to_datetime(test.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title_event_code'] = train['title'].astype(str) + '_' + train['event_code'].astype(str)\n",
    "test['title_event_code'] = test['title'].astype(str) + '_' + test['event_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_event_code = set(train.event_code).union(set(test.event_code))\n",
    "list_of_event_id = set(train.event_id).union(set(test.event_id))\n",
    "list_title = set(train.title).union(set(test.title))\n",
    "list_title_event_code = set(train.title_event_code).union(set(test.title_event_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting(test_sample):\n",
    "    pred = [] \n",
    "    for i in range(test_sample.shape[1]):\n",
    "        result = test_sample[:,i]\n",
    "        value_counts = pd.Series(result).value_counts()\n",
    "        pred_value = np.random.permutation(list(value_counts.index[value_counts==value_counts.iloc[0]]))[0]\n",
    "        pred.append(pred_value)\n",
    "    return np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_lgb(X_train,y_train,ins_id,X_test,seed):\n",
    "    coefficients = [1.051, 1.682, 2.231]\n",
    "    gkf = GroupKFold(n_splits=4)\n",
    "    impor1 = 0\n",
    "    score = 0\n",
    "    score_kappa = 0\n",
    "    models = []\n",
    "    oof_predict = np.zeros((y_train.shape[0]))\n",
    "    pred_test1 = 0\n",
    "    for train_index, test_index in gkf.split(X_train, y_train,groups=ins_id):\n",
    "        X_train2= X_train.iloc[train_index,:]\n",
    "        y_train2= y_train.iloc[train_index]\n",
    "        X_test2= X_train.iloc[test_index,:]\n",
    "        y_test2= y_train.iloc[test_index]\n",
    "        clf = lgb.LGBMRegressor(n_estimators=10000, random_state=seed,learning_rate=0.01,importance_type = 'gain',\n",
    "                          n_jobs = -1,num_leaves=17,bagging_freq=1,metric = 'rmse',colsample_bytree=0.7675954818362201\n",
    "                                ,subsample=0.889919359551312,min_child_weight = 1.8540153787292635,\n",
    "                                  min_child_samples = 20,reg_alpha = 2.0248784225430105,reg_lambda = 0.010193000624989956,\n",
    "                               min_split_gain =0.03265285313891397 )\n",
    "        clf.fit(X_train2,y_train2,eval_set = [(X_train2,y_train2),(X_test2,y_test2)],early_stopping_rounds=200,verbose=0,\n",
    "               categorical_feature=['title','world'])#,eval_metric = CRPS_eval)\n",
    "        models.append(clf)\n",
    "        temp_predict = clf.predict(X_test2)\n",
    "        pred_test1 +=  clf.predict(X_test)/gkf.n_splits\n",
    "        oof_predict[test_index] = temp_predict\n",
    "        rse_1 = np.sqrt(mean_squared_error(y_test2,temp_predict))#,squared=False)\n",
    "        preds = temp_predict.copy()\n",
    "        preds[preds <= coefficients[0]] = 0\n",
    "        preds[np.where(np.logical_and(preds > coefficients[0], preds <= coefficients[1]))] = 1\n",
    "        preds[np.where(np.logical_and(preds > coefficients[1], preds <= coefficients[2]))] = 2\n",
    "        preds[preds > coefficients[2]] = 3\n",
    "        kappa_1 = qwk(y_test2, preds)\n",
    "        score += rse_1/gkf.n_splits\n",
    "        score_kappa += kappa_1/gkf.n_splits\n",
    "        impor1 += clf.feature_importances_/gkf.n_splits\n",
    "        gc.collect()\n",
    "    print('mean rse:',score)\n",
    "    print('mean kappa:',score_kappa)\n",
    "    preds = oof_predict.copy()\n",
    "    preds[preds <= coefficients[0]] = 0\n",
    "    preds[np.where(np.logical_and(preds > coefficients[0], preds <= coefficients[1]))] = 1\n",
    "    preds[np.where(np.logical_and(preds > coefficients[1], preds <= coefficients[2]))] = 2\n",
    "    preds[preds > coefficients[2]] = 3\n",
    "    print('oof kappa:',qwk(y_train, preds))\n",
    "    #### oof_num,oof_class, pred_num,impor\n",
    "    return oof_predict,preds,pred_test1,impor1,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_lgb(X_train,y_train,ins_id,X_test):\n",
    "    cappa_list = []\n",
    "    oof_class_list = []\n",
    "    rmse_list = []\n",
    "    impor_list = []\n",
    "    pred_class_list = []\n",
    "    oof_num_mean = 0\n",
    "    pred_num_pred = 0\n",
    "    temp_score_overall = []\n",
    "    \n",
    "    seed_list = [7,8,9,10,11]\n",
    "    for kth,seed1 in enumerate(seed_list):#[32,42,47,1103,128,22,45][11,22,33,43,45,56,67]\n",
    "        print(str(kth+1),'th seed begins----------------------' )\n",
    "        oof_num,oof_class,pred_num,impor1,rmse = normal_lgb(X_train,y_train,ins_id,X_test,seed1)\n",
    "        pred_num_pred += pred_num/len(seed_list)\n",
    "        coefficients = [1.051, 1.682, 2.231]\n",
    "        temp_inter = 0\n",
    "        temp_score = 0 \n",
    "        \n",
    "        for i in range(400):###100可以继续往上加\n",
    "            t,score = opt_more_coeff(oof_num[list(truncated_groups[:,i*2+seed1])],y_train.loc[list(truncated_groups[:,i*2+seed1])],[1.051, 1.682, 2.231])\n",
    "\n",
    "            temp_inter +=np.array(t)/400\n",
    "            temp_score +=score/400\n",
    "        print(temp_inter)\n",
    "        print('mean cappa',temp_score)\n",
    "        temp_score_overall.append(temp_score)\n",
    "        \n",
    "        coefficients2 = list(temp_inter)\n",
    "        print('New threshold',coefficients2)\n",
    "        y_pred_class = get_pred(pred_num,coefficients2)\n",
    "        print(pd.Series(y_pred_class).value_counts())\n",
    "        cappa1 = qwk(y_train,get_pred(oof_num,coefficients2))\n",
    "        cappa_list.append(cappa1)\n",
    "        oof_class = get_pred(oof_num,coefficients2)\n",
    "        oof_class_list.append(oof_class)\n",
    "        oof_num_mean += oof_num/len(seed_list)\n",
    "        rmse_list.append(rmse)\n",
    "        impor_list.append(impor1)\n",
    "        pred_class_list.append(y_pred_class)\n",
    "    print('Overall situation-----------------------')\n",
    "    print('Mean rmse {}'.format(round(np.mean(rmse_list),5)))\n",
    "    print('Mean cappa {}'.format(round(np.mean(temp_score_overall),5)))\n",
    "    impor2 = 0\n",
    "    for _ in impor_list:\n",
    "        impor2 += _/len(impor_list)\n",
    "    return voting(np.vstack(tuple(pred_class_list))),impor2,voting(np.vstack(tuple(oof_class_list))),oof_num_mean,pred_num_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_time = pd.read_csv('/kaggle/input/dsb-media-sequence/media_sequence.csv')\n",
    "clip_time = clip_time[clip_time.type == 'Clip']\n",
    "clip_time= clip_time[['title','duration']].set_index('title')['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_list = ['Cart Balancer (Assessment)',\n",
    " 'Cauldron Filler (Assessment)',\n",
    " 'Mushroom Sorter (Assessment)',\n",
    " 'Chest Sorter (Assessment)',\n",
    " 'Bird Measurer (Assessment)']\n",
    "\n",
    "world_list = ['CRYSTALCAVES','MAGMAPEAK','TREETOPCITY']\n",
    "\n",
    "\n",
    "duration_event_id = ['90efca10', '4ef8cdd3', '9ee1c98c', 'e694a35b', '08fd73f3', '2dc29e21',\n",
    "       '499edb7c', 'e9c52111', 'd185d3ea', 'd3f1e122', 'cdd22e43', 'fcfdffb6',\n",
    "       '84538528', 'c58186bf', 'de26c3a6', '5c2f29ca', '598f4598', '804ee27f',\n",
    "       '56817e2b', '37c53127', '4a4c3d21', '363c86c9', 'e7561dd2', '022b4259',\n",
    "       '5a848010', '562cec5f', '71e712d8', 'd02b7a8e', '30614231', '3323d7e9',\n",
    "       '5f0eb72c', '0db6d71d', '4a09ace1', 'ca11f653', '1c178d24', '46cd75b4',\n",
    "       '00c73085', 'bc8f2793', '36fa3ebe', '16dffff1', '8fee50e2', '392e14df',\n",
    "       '9d4e7b25', 'd06f75b5', '895865f3', 'c1cac9a2', '28520915', 'd51b1749',\n",
    "       'b012cd7f', 'f5b8c21a', '3d0b9317', '9ce586dd', '3d63345e', 'c7128948',\n",
    "       '5348fd84', '83c6c409', '99abe2bb', 'c189aaf2', '99ea62f3', 'b74258a0',\n",
    "       'f6947f54', '6c930e6e', '6088b756', '9ed8f6da', '3edf6747', '53c6e11a',\n",
    "       'a1192f43', '4d6737eb', 'd2659ab4', 'd38c2fd7', '3bb91ced', 'd88ca108',\n",
    "       'a76029ee', '38074c54', '86ba578b', '0d18d96c']\n",
    "\n",
    "misses_id = ['08fd73f3', '56817e2b', '37c53127', '3323d7e9', 'ca11f653', '1c178d24',\n",
    "       '00c73085', '36fa3ebe', '16dffff1', '895865f3', '28520915', 'b012cd7f',\n",
    "       'f5b8c21a', 'b74258a0', 'f6947f54', '6c930e6e', '38074c54']\n",
    "\n",
    "activity_list = ['Bottle Filler (Activity)', 'Sandcastle Builder (Activity)',\n",
    "       'Chicken Balancer (Activity)', 'Fireworks (Activity)',\n",
    "       'Flower Waterer (Activity)', 'Bug Measurer (Activity)',\n",
    "       'Egg Dropper (Activity)', 'Watering Hole (Activity)']\n",
    "\n",
    "list_4020_4030 = {'Bottle Filler (Activity)':[4020,4030],\n",
    "                 'Bug Measurer (Activity)':[4030],\n",
    "                 'Chicken Balancer (Activity)':[4020,4030],\n",
    "                 'Egg Dropper (Activity)':[4020],\n",
    "                 'Fireworks (Activity)':[4020,4030],\n",
    "                 'Flower Waterer (Activity)':[4020,4030],\n",
    "                 'Sandcastle Builder (Activity)':[4020,4030],\n",
    "                 'Watering Hole (Activity)':[4020]}\n",
    "\n",
    "game_list = ['Chow Time', 'Scrub-A-Dub', 'All Star Sorting', 'Dino Drink',\n",
    "       'Bubble Bath', 'Crystals Rule', 'Dino Dive', 'Pan Balance',\n",
    "       'Happy Camel', 'Air Show', 'Leaf Leader']\n",
    "\n",
    "def get_data(user_sample,test_set = False):\n",
    "    \n",
    "\n",
    "    duration_id_list = {eve:0 for eve in duration_event_id}\n",
    "    duration_id_count = {eve:0 for eve in duration_event_id}\n",
    "    clip_abandon = 0\n",
    "    \n",
    "    activity_abandon_dict = {eve:0 for eve in activity_list}\n",
    "    activity_abandon_cnt = {eve:0 for eve in activity_list}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #### round\n",
    "    num_round_list = []\n",
    "    num_round_max_list = []\n",
    "\n",
    "    round_title_max_dict = {'round_Scrub-A-Dub_max':[], \n",
    "                         'round_Dino Drink_max':[], \n",
    "                         'round_All Star Sorting_max':[], \n",
    "                         'round_Air Show_max':[],\n",
    "                        'round_Crystals Rule_max':[],\n",
    "                         'round_Bubble Bath_max':[], \n",
    "                         'round_Bottle Filler (Activity)_max':[],\n",
    "                        'round_Dino Dive_max':[], \n",
    "                         'round_Chow Time_max':[], \n",
    "                         'round_Pan Balance_max':[], \n",
    "                         'round_Happy Camel_max':[],\n",
    "                        'round_Leaf Leader_max':[]}    \n",
    "    \n",
    "    #### misss\n",
    "    num_miss = []\n",
    "    miss_id_list = {eve:[] for eve in misses_id}\n",
    "    miss_37c53127_dict = {'123':[],'456':[],'789':[],'other_level':[]}\n",
    "    level_37c53127_list = []\n",
    "    level_37c53127_list_max = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###加时间\n",
    "    count_game_diff_assessment = 0\n",
    "    count_assessment_diff_assessment = 0 \n",
    "    count_activity_diff_assessment = 0 \n",
    "    count_clip_diff_assessment = 0\n",
    "    \n",
    "    last_world = -1\n",
    "    ###1是计算是不是有直接开始assessment\n",
    "    shangyici_world_time1 = {'syc_MAGMAPEAK':[],'syc_TREETOPCITY':[],'syc_CRYSTALCAVES':[]}\n",
    "    shangyici_world_time2 = {'syc_MAGMAPEAK':[],'syc_TREETOPCITY':[],'syc_CRYSTALCAVES':[]}\n",
    "    shangyici_test_time = {'syc_Mushroom Sorter (Assessment)':[],'syc_Chest Sorter (Assessment)':[],'syc_Bird Measurer (Assessment)':[],\n",
    "                           'syc_Cauldron Filler (Assessment)':[],'syc_Cart Balancer (Assessment)':[]}\n",
    "    #accumulated_actions = 0\n",
    "    time_in_session = -1\n",
    "    durations = []\n",
    "    all_times = 0\n",
    "    count_type_dict = {'count_Game':0,'count_Assessment':0,'count_Activity':0,'count_Clip':0}\n",
    "    count_world_dict = {'count_MAGMAPEAK':0,'count_TREETOPCITY':0,'count_CRYSTALCAVES':0}\n",
    "    time_type_dict = {'time_Game':0,'time_Assessment':0,'time_Activity':0,'time_Clip':0}\n",
    "    time_world_dict = {'time_MAGMAPEAK':0,'time_TREETOPCITY':0,'time_CRYSTALCAVES':0}\n",
    "    count_session = 0\n",
    "    num_th_assess = 0\n",
    "    all_features = []\n",
    "    child_id = user_sample.installation_id.values[0]\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    abandon_times = 0\n",
    "    game_abandon = 0\n",
    "    games_all = 0##game_all 会统一统计 可以删去\n",
    "    num_game_correct = 0\n",
    "    num_game_false = 0\n",
    "    accum_avg_acc=[]\n",
    "    accum_avg_group = []\n",
    "    times_game_no_success = 0\n",
    "    times_game_success = 0\n",
    "    game_correct_list = []##acc_list\n",
    "    num_enter_game = 0\n",
    "    enter_game_acc_list = []\n",
    "    enter_game_num_corr = 0\n",
    "    enter_game_num_false = 0\n",
    "    grp_assess_list = {'grp_Bird Measurer (Assessment)':[],'grp_Chest Sorter (Assessment)':[],\\\n",
    "                              'grp_Cauldron Filler (Assessment)':[],'grp_Cart Balancer (Assessment)':[],\\\n",
    "                              'grp_Mushroom Sorter (Assessment)':[]}\n",
    "    acc_assess_list = {'acc_Bird Measurer (Assessment)':[],'acc_Chest Sorter (Assessment)':[],\\\n",
    "                              'acc_Cauldron Filler (Assessment)':[],'acc_Cart Balancer (Assessment)':[],\\\n",
    "                              'acc_Mushroom Sorter (Assessment)':[]}\n",
    "    curr_world_type_time={'time_Game_curr_world':0,'time_Assessment_curr_world':0,'time_Activity_curr_world':0,'time_Clip_curr_world':0}\n",
    "    curr_world_type_cnt = {'count_Game_curr_world':0,'count_Assessment_curr_world':0,'count_Activity_curr_world':0,'count_Clip_curr_world':0}\n",
    "    curr_world_type_time_all = 0\n",
    "    curr_world_type_count_all = 0\n",
    "    event_code_count = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count = {eve: 0 for eve in list_title}\n",
    "    title_event_code_count = {t_eve: 0 for t_eve in list_title_event_code}\n",
    "\n",
    "    \n",
    "    for i,session in user_sample.groupby('game_session',sort=False):\n",
    "        session_type = session.type.values[0]\n",
    "        session_title = session.title.values[0]\n",
    "        session_world = session.world.values[0]\n",
    "        session_time_stamp_min = session.timestamp.min()\n",
    "        session_time_stamp_max = session.timestamp.max()\n",
    "        \n",
    "        if session_title == 'Welcome to Lost Lagoon!':\n",
    "            num_enter_game +=1\n",
    "            enter_game_acc_list = []\n",
    "            enter_game_num_corr = 0\n",
    "            enter_game_num_false = 0\n",
    "        \n",
    "        if session_type == 'Game':\n",
    "            games_all +=1\n",
    "            game_assess_df = session[session.event_data.str.contains('\"correct\":')].copy()\n",
    "            if game_assess_df.shape[0] >0:\n",
    "                num_corr = np.sum(session.event_data.str.contains('\"correct\":true').astype(int))\n",
    "                num_false = np.sum(session.event_data.str.contains('\"correct\":false').astype(int))\n",
    "                num_game_correct += num_corr\n",
    "                num_game_false += num_false\n",
    "                enter_game_num_corr += num_corr\n",
    "                enter_game_num_false +=num_false\n",
    "                game_correct_list.append(num_corr/(num_corr + num_false))\n",
    "                enter_game_acc_list.append(num_corr/(num_corr + num_false))\n",
    "                if num_false>0 and num_corr==0:\n",
    "                    times_game_no_success +=1\n",
    "                if num_corr >0:\n",
    "                    times_game_success +=1\n",
    "            else:\n",
    "                game_abandon +=1\n",
    "             \n",
    "        \n",
    "        if session_type == 'Assessment':\n",
    "            session_attempt = session[((session.event_code==4100)&(session.title != 'Bird Measurer (Assessment)'))\\\n",
    "              |((session.event_code==4110)&(session.title == 'Bird Measurer (Assessment)'))]\n",
    "            mark = (test_set & (len(session)==1) & (session.event_code.values[0]==2000))\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "            if (session_attempt.shape[0]>0)|mark:\n",
    "                shangyici_test_time['syc_'+session_title].append(session_time_stamp_min)\n",
    "                shangyici_world_time1['syc_'+session_world].append(session_time_stamp_min)\n",
    "                features = dict()\n",
    "                ####miss\n",
    "                features['sum_num_miss'] = np.sum(num_miss) if len(num_miss)!=0 else -1  \n",
    "                features['mean_num_miss'] = np.mean(num_miss) if len(num_miss)!=0 else -1\n",
    "                temp_dict = {'mean_num_miss'+key:np.mean(values) if len(values)!=0 else -1 for key,values in miss_id_list.items()}\n",
    "                features.update(temp_dict)\n",
    "                temp_dict = {'sum_num_miss'+key:np.sum(values) if len(values)!=0 else -1 for key,values in miss_id_list.items()}\n",
    "                features.update(temp_dict)\n",
    "                temp_dict = {'last_5_sum_num_miss'+key:np.sum(values[-5:]) if len(values)!=0 else -1 for key,values in miss_id_list.items()}\n",
    "                features.update(temp_dict)\n",
    "#                 temp_dict = {'mean_num_miss_level_37c53127'+key:np.mean(values) if len(values)!=0 else -1 for key,values in miss_37c53127_dict.items()}\n",
    "#                 features.update(temp_dict)\n",
    "#                 temp_dict = {'sum_num_miss_level_37c53127'+key:np.sum(values) if len(values)!=0 else -1 for key,values in miss_37c53127_dict.items()}\n",
    "#                 features.update(temp_dict)\n",
    "#                 temp_dict = {'length_num_miss_level_37c53127'+key:len(values) for key,values in miss_37c53127_dict.items()}\n",
    "#                 features.update(temp_dict)\n",
    "                features['mean_level_37c53127'] = np.mean(level_37c53127_list) if len(level_37c53127_list)!=0 else -1\n",
    "                features['mean_level_37c53127_max'] = np.mean(level_37c53127_list_max) if len(level_37c53127_list_max)!=0 else -1\n",
    "#                 features['sum_level_37c53127_max'] = np.sum(level_37c53127_list_max) if len(level_37c53127_list_max)!=0 else -1\n",
    "                \n",
    "                #####\n",
    "\n",
    "                temp_dict = {'overall_mean_'+key: values/duration_id_count[key] if duration_id_count[key]!=0 else -1 for key,values in duration_id_list.items()}\n",
    "                features.update(temp_dict)\n",
    "                temp_dict = {'overall_sum_'+key:values for key,values in duration_id_list.items()}\n",
    "                features.update(temp_dict)\n",
    "\n",
    "                \n",
    "                \n",
    "                features['num_round_list'] = np.mean(num_round_list) if len(num_round_list)!=0 else -1\n",
    "                features['num_round_max_list'] = np.mean(num_round_max_list) if len(num_round_max_list)!=0 else -1\n",
    "                features['num_round_max_sum'] = np.sum(num_round_max_list) if len(num_round_max_list)!=0 else -1\n",
    "                features['num_round_max_std'] = np.std(num_round_max_list) if len(num_round_max_list)!=0 else -1\n",
    "\n",
    "\n",
    "                temp_round_max_title = {'mean1_'+key:np.mean(value) if len(value)!=0 else -1 for key,value in round_title_max_dict.items()}\n",
    "                features.update(temp_round_max_title.copy())\n",
    "                temp_round_max_title = {'sum1_'+key:np.sum(value) if len(value)!=0 else -1 for key,value in round_title_max_dict.items()}\n",
    "                features.update(temp_round_max_title.copy())\n",
    "                \n",
    "                #features['accumulated_actions'] = accumulated_actions\n",
    "                features['last_session_time'] = time_in_session\n",
    "                #features['all_times']= all_times\n",
    "                features['duration_mean'] = np.mean(durations) if len(durations)!=0 else -1\n",
    "                features['duration_std'] = np.std(durations) if len(durations)!=0 else -1\n",
    "                features['mean_game_acc'] = np.mean(game_correct_list) if len(game_correct_list)!=0 else -1\n",
    "                features['std_game_acc'] = np.std(game_correct_list) if len(game_correct_list)!=0 else -1\n",
    "                features['mean_enter_game_acc'] = np.mean(enter_game_acc_list) if len(enter_game_acc_list)!=0 else -1\n",
    "                features['std_enter_game_acc'] = np.std(enter_game_acc_list) if len(enter_game_acc_list)!=0 else -1\n",
    "                #features['last_5_mean_game_acc'] = np.mean(game_correct_list[-5:]) if len(game_correct_list)!=0 else -1\n",
    "                features['last_1_mean_game_acc'] = game_correct_list[-1] if len(game_correct_list)!=0 else -1\n",
    "                features['count_game_acc'] = len(game_correct_list)\n",
    "                features['num_enter_game'] = num_enter_game\n",
    "                features.update(event_code_count.copy())\n",
    "                features.update(event_id_count.copy())\n",
    "                features.update(title_count.copy())\n",
    "                features.update(title_event_code_count.copy())\n",
    "                features.update(count_type_dict.copy())\n",
    "                features.update(count_world_dict.copy())\n",
    "                features['clip_abandon'] = clip_abandon\n",
    "                features['clip_abandon_ratio'] = clip_abandon/features['count_Clip'] if features['count_Clip']!=0 else -1\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "                temp_dict = {'abandon_ratio_'+eve:values/activity_abandon_cnt[eve] if activity_abandon_cnt[eve]!=0 else -1 \\\n",
    "                             for eve,values in activity_abandon_dict.items()}\n",
    "                features.update(temp_dict)\n",
    "                \n",
    "                temp_dict = {'abandon_cnt_'+eve:values for eve,values in activity_abandon_dict.items()}\n",
    "                features.update(temp_dict)\n",
    "                \n",
    "                \n",
    "                if session_world!='NONE':\n",
    "                    features['curr_world_time'] = time_world_dict['time_'+session_world]\n",
    "                    #features['curr_world_time_per'] = time_world_dict['time_'+session_world]/all_times if all_times!=0 else -1\n",
    "                features['count_session'] = count_session\n",
    "                features.update(time_type_dict.copy())\n",
    "                features.update(time_world_dict.copy())\n",
    "                \n",
    "                temp_acc_assess_list = {key: np.mean(values) if len(values)!=0 else -1 for key,values in acc_assess_list.items()}\n",
    "                features.update(temp_acc_assess_list)\n",
    "\n",
    "\n",
    "                #features['times_game_no_success'] = times_game_no_success\n",
    "                #features['times_game_success'] = times_game_success\n",
    "                features['ratio_time_game_success'] = times_game_success/(times_game_success + times_game_no_success) if (times_game_success + times_game_no_success)!=0 else -1\n",
    "                features.update(accuracy_groups)\n",
    "                temp_all = accuracy_groups[0] + accuracy_groups[1] + accuracy_groups[2] + accuracy_groups[3]\n",
    "                if temp_all==0:\n",
    "                    features['0_ratio'] = 0\n",
    "                    features['1_ratio'] = 0\n",
    "                    features['2_ratio'] = 0\n",
    "                    features['3_ratio'] = 0\n",
    "                else:\n",
    "                    features['0_ratio'] = accuracy_groups[0]/temp_all\n",
    "                    features['1_ratio'] = accuracy_groups[1]/temp_all\n",
    "                    features['2_ratio'] = accuracy_groups[2]/temp_all\n",
    "                    features['3_ratio'] = accuracy_groups[3] /temp_all  \n",
    "                features['installation_id'] = child_id\n",
    "                features['game_session'] = i\n",
    "                features['title'] = session_title\n",
    "                features['world'] = session_world\n",
    "                features['num_th_assess'] = num_th_assess\n",
    "                features['abandon_times'] = abandon_times\n",
    "                num_th_assess += 1\n",
    "                features['accum_avg_acc'] = np.mean(accum_avg_acc) if len(accum_avg_acc)!=0 else -1\n",
    "                features['accum_avg_group'] = np.mean(accum_avg_group) if len(accum_avg_group)!=0 else -1\n",
    "                features['accum_std_acc'] = np.std(accum_avg_acc) if len(accum_avg_acc)!=0 else -1\n",
    "                features['accum_std_group'] = np.std(accum_avg_group) if len(accum_avg_group)!=0 else -1\n",
    "\n",
    "                features['last_5_accum_avg_acc'] = np.mean(accum_avg_acc[-5:]) if len(accum_avg_acc)!=0 else -1\n",
    "                #features['last_5_accum_avg_group'] = np.mean(accum_avg_group[-5:]) if len(accum_avg_group)!=0 else -1\n",
    "                features['last_5_accum_std_acc'] = np.std(accum_avg_acc[-5:]) if len(accum_avg_acc)!=0 else -1\n",
    "                features['last_5_accum_std_group'] = np.std(accum_avg_group[-5:]) if len(accum_avg_group)!=0 else -1\n",
    "                #features['last_1_accum_avg_acc'] = accum_avg_acc[-1] if len(accum_avg_acc)!=0 else -1\n",
    "                features['last_1_accum_avg_group'] = accum_avg_group[-1] if len(accum_avg_group)!=0 else -1\n",
    "                ###加这两个的count\n",
    "                features['overall_game_corr_ratio'] = num_game_correct/(num_game_correct + num_game_false) if (num_game_correct + num_game_false)!=0 else -1\n",
    "                features['enter_game_overall_game_corr_ratio'] = enter_game_num_corr/(enter_game_num_corr + enter_game_num_false) if (enter_game_num_corr + enter_game_num_false) !=0 else -1\n",
    "\n",
    "                \n",
    "                features['game_abandon'] = game_abandon\n",
    "                features['game_abandon_ratio'] = game_abandon/games_all if games_all!=0 else -1\n",
    "                features['mean_same_assess_grp'] = np.mean(grp_assess_list['grp_'+session_title]) if len(grp_assess_list['grp_'+session_title])!=0 else -1\n",
    "                features['mean_same_assess_acc'] = np.mean(acc_assess_list['acc_'+session_title]) if len(acc_assess_list['acc_'+session_title])!=0 else -1\n",
    "                features['std_same_assess_grp'] = np.std(grp_assess_list['grp_'+session_title]) if len(grp_assess_list['grp_'+session_title])!=0 else -1\n",
    "                features['std_same_assess_acc'] = np.std(acc_assess_list['acc_'+session_title]) if len(acc_assess_list['acc_'+session_title])!=0 else -1\n",
    "                features['count_same_assess_grp'] = len(grp_assess_list['grp_'+session_title])\n",
    "\n",
    "                features['last_3_mean_same_assess_grp'] = np.mean(grp_assess_list['grp_'+session_title][-3:]) if len(grp_assess_list['grp_'+session_title])!=0 else -1\n",
    "                features['last_3_mean_same_assess_acc'] = np.mean(acc_assess_list['acc_'+session_title][-3:]) if len(acc_assess_list['acc_'+session_title])!=0 else -1\n",
    "                \n",
    "                features['last_1_mean_same_assess_grp'] = grp_assess_list['grp_'+session_title][-1] if len(grp_assess_list['grp_'+session_title])!=0 else -1\n",
    "                features['last_1_mean_same_assess_acc'] = acc_assess_list['acc_'+session_title][-1] if len(acc_assess_list['acc_'+session_title])!=0 else -1\n",
    "                \n",
    "                features['last_test_to_now_time'] = (shangyici_test_time['syc_'+session_title][-1] \\\n",
    "                                                     - shangyici_test_time['syc_'+session_title][-2]).total_seconds() if len(shangyici_test_time['syc_'+session_title])>=2 else -1\n",
    "                features['last_world_to_now_time1'] =  (shangyici_world_time1['syc_'+session_world][-1]\\\n",
    "                                                        - shangyici_world_time1['syc_'+session_world][-2]).total_seconds() if len(shangyici_world_time1['syc_'+session_world])>=2 else -1\n",
    "                features['last_world_to_now_time2'] =  (shangyici_world_time2['syc_'+session_world][-1]\\\n",
    "                                                        - shangyici_world_time2['syc_'+session_world][-2]).total_seconds() if len(shangyici_world_time2['syc_'+session_world])>=2 else -1\n",
    "                features['count_game_diff_assessment'] = count_game_diff_assessment\n",
    "                features['count_assessment_diff_assessment'] = count_assessment_diff_assessment\n",
    "                features['count_activity_diff_assessment'] = count_activity_diff_assessment\n",
    "                features['count_clip_diff_assessment'] = count_clip_diff_assessment\n",
    "                features.update(curr_world_type_time.copy())\n",
    "                features.update(curr_world_type_cnt.copy())\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 features['curr_world_type_time_all'] = curr_world_type_time_all\n",
    "#                 features['curr_world_type_count_all'] = curr_world_type_count_all\n",
    "                if not mark:\n",
    "                    true_attempt = np.sum(session_attempt.event_data.str.contains('\"correct\":true').astype(int))\n",
    "                    false_attempt = np.sum(session_attempt.event_data.str.contains('\"correct\":false').astype(int))\n",
    "                    features['accuracy'] = true_attempt/(true_attempt + false_attempt) if (true_attempt + false_attempt)!=0 else 0\n",
    "                    features['accuracy_group'] = to_accuracy_group(features['accuracy'])\n",
    "                    accum_avg_acc.append(features['accuracy'])\n",
    "                    accum_avg_group.append(features['accuracy_group'])\n",
    "                    accuracy_groups[features['accuracy_group']] += 1\n",
    "                    grp_assess_list['grp_'+session_title].append(features['accuracy_group'])\n",
    "                    acc_assess_list['acc_'+session_title].append(features['accuracy'])\n",
    "                all_features.append(features)\n",
    "                count_game_diff_assessment = 0\n",
    "                count_assessment_diff_assessment = 0\n",
    "                count_activity_diff_assessment = 0\n",
    "                count_clip_diff_assessment = 0\n",
    "            else:\n",
    "                abandon_times +=1\n",
    "            \n",
    "        def update_counters(counter, col):\n",
    "            num_of_session_count = Counter(session[col])\n",
    "            for k in num_of_session_count.keys():\n",
    "                x = k\n",
    "                counter[x] += num_of_session_count[k]\n",
    "            return counter\n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count,'title_event_code')\n",
    "        count_session +=1\n",
    "        count_type_dict['count_'+session_type] +=1\n",
    "        if session_type != 'Clip':\n",
    "            time_in_session = (session_time_stamp_max - session_time_stamp_min).total_seconds()\n",
    "        else:\n",
    "            try:\n",
    "                temp_time = session.timestamp.min()\n",
    "                temp_index = session.index[0]+1\n",
    "                time_in_session = ((user_sample.loc[temp_index,'timestamp']) - temp_time).total_seconds()\n",
    "                if time_in_session >clip_time[session_title]:\n",
    "                    time_in_session = clip_time[session_title]\n",
    "            except:\n",
    "                time_in_session = clip_time[session_title]\n",
    "            if time_in_session < 0.5*clip_time[session_title]:\n",
    "                clip_abandon +=1\n",
    "        time_type_dict['time_'+session_type] += time_in_session\n",
    "        durations.append(time_in_session)\n",
    "        all_times +=time_in_session\n",
    "        if session_world!='NONE':\n",
    "            count_world_dict['count_'+session_world] +=1\n",
    "            time_world_dict['time_'+session_world] += time_in_session\n",
    "        #accumulated_actions += len(session)\n",
    "        \n",
    "        if session_world!=last_world:\n",
    "            curr_world_type_time={'time_Game_curr_world':0,'time_Assessment_curr_world':0,'time_Activity_curr_world':0,'time_Clip_curr_world':0}\n",
    "            curr_world_type_cnt = {'count_Game_curr_world':0,'count_Assessment_curr_world':0,'count_Activity_curr_world':0,'count_Clip_curr_world':0}\n",
    "#             curr_world_type_time_all = 0\n",
    "#             curr_world_type_count_all = 0\n",
    "            if session_world != 'NONE':\n",
    "                shangyici_world_time2['syc_'+session_world].append(session_time_stamp_min)\n",
    "            last_world = session_world\n",
    "        if session_world != 'NONE':\n",
    "#             curr_world_type_time_all +=time_in_session\n",
    "#             curr_world_type_count_all +=1\n",
    "            curr_world_type_time['time_'+session_type+'_curr_world'] +=time_in_session\n",
    "            curr_world_type_cnt['count_'+session_type+'_curr_world'] +=1\n",
    "        #####1230 count4070系列\n",
    "        \n",
    "        if session_type == 'Game':\n",
    "            count_game_diff_assessment +=1\n",
    "        if session_type == 'Activity':\n",
    "            count_activity_diff_assessment+=1\n",
    "        if session_type == 'Assessment':\n",
    "            count_assessment_diff_assessment+=1\n",
    "        if session_type =='Clip':\n",
    "            count_clip_diff_assessment +=1\n",
    "        session_miss = session[session.event_data.str.contains('\"misses\":')].copy()\n",
    "        if session_miss.shape[0]>0:\n",
    "            session_miss['num_miss'] = session_miss.event_data.map(lambda x:json.loads(x)['misses'])\n",
    "            for id1,miss in zip(session_miss['event_id'],session_miss['num_miss']):\n",
    "                num_miss.append(miss)\n",
    "                miss_id_list[id1].append(miss)\n",
    "            session_37c53127 = session_miss[session_miss.event_id=='37c53127']\n",
    "            if session_37c53127.shape[0]>0:\n",
    "                session_37c53127['level'] = session_37c53127.event_data.map(lambda x:json.loads(x)['level'])\n",
    "                level_37c53127_list_max.append(np.max(session_37c53127['level']))\n",
    "                for level,miss in zip(session_37c53127['level'],session_37c53127['num_miss']):\n",
    "                    level_37c53127_list.append(level)\n",
    "#                     if level in [1,2,3]:\n",
    "#                         miss_37c53127_dict['123'].append(miss)\n",
    "#                     elif level in [4,5,6]:\n",
    "#                         miss_37c53127_dict['456'].append(miss)\n",
    "#                     elif level in [7,8,9]:\n",
    "#                         miss_37c53127_dict['789'].append(miss)\n",
    "#                     else:\n",
    "#                         miss_37c53127_dict['other_level'].append(miss)\n",
    "                    \n",
    "\n",
    "        ####round\n",
    "        session_round = session[(session.event_data.str.contains('\"round\":'))&(session.event_code==2030)].copy()\n",
    "        if session_round.shape[0]>0:\n",
    "            session_round = session[session.event_data.str.contains('\"round\":')].copy()\n",
    "            session_round['num_round'] = session_round.event_data.map(lambda x:json.loads(x)['round'])\n",
    "            num_round_list.extend(list(session_round.num_round))\n",
    "            num_round_max_list.append(np.max(session_round.num_round))\n",
    "            round_title_max_dict['round_'+session_title+'_max'].append(np.max(session_round.num_round))\n",
    "        if np.sum(session.event_data.str.contains('\"duration\":'))>0:\n",
    "            session_duration = session[(session.event_data.str.contains('\"duration\":'))&(session.event_id.map(lambda x:x in duration_event_id))].copy()\n",
    "            if session_duration.shape[0]>0:\n",
    "                session_duration['duration'] = session_duration.event_data.map(lambda x:json.loads(x)['duration'])\n",
    "                for id1,duration in zip(session_duration['event_id'],session_duration['duration']):\n",
    "\n",
    "                    duration_id_list[id1] += duration\n",
    "                    duration_id_count[id1] += 1\n",
    "        if session_type=='Activity':\n",
    "            activity_abandon_cnt[session_title]+=1\n",
    "            if session_title in ['Bottle Filler (Activity)','Chicken Balancer (Activity)',\n",
    "                                'Fireworks (Activity)','Flower Waterer (Activity)','Sandcastle Builder (Activity)']:\n",
    "                num_4020 = np.sum(session.event_code==4020)\n",
    "                num_4030 = np.sum(session.event_code==4030)\n",
    "                if num_4020==0 or num_4030 ==0:\n",
    "                    activity_abandon_dict[session_title]+=1\n",
    "            elif session_title in ['Bug Measurer (Activity)']:\n",
    "                num_4030 = np.sum(session.event_code==4030)\n",
    "                if num_4030 ==0:\n",
    "                    activity_abandon_dict[session_title]+=1\n",
    "            elif session_title in ['Egg Dropper (Activity)','Watering Hole (Activity)']:\n",
    "                num_4020 = np.sum(session.event_code==4020)\n",
    "                if num_4020 ==0:\n",
    "                    activity_abandon_dict[session_title]+=1\n",
    "\n",
    "\n",
    "\n",
    "    if test_set:\n",
    "        return all_features[-1]    \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17000/17000 [53:54<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_train(train):\n",
    "    train_all = []\n",
    "    for ins_id,user_sample in tqdm(train.groupby('installation_id',sort=False)):\n",
    "        train_all.extend(get_data(user_sample))\n",
    "    return pd.DataFrame(train_all)\n",
    "train_all=get_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:12<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_test(test):\n",
    "    test_all = []\n",
    "    for ins_id,user_sample in tqdm(test.groupby('installation_id',sort=False)):\n",
    "        test_all.append(get_data(user_sample,test_set = True))\n",
    "    return pd.DataFrame(test_all)\n",
    "\n",
    "test_all = get_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_all.accuracy_group.copy()\n",
    "drop_col = ['accuracy','game_session','accuracy_group','installation_id']\n",
    "ins_id = train_all.installation_id\n",
    "X_train = train_all.drop(drop_col,axis=1)\n",
    "X_test = test_all.drop(['game_session','installation_id'],axis=1)\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "X_train = X_train[sorted(X_train.columns.tolist())]\n",
    "X_test = X_test[sorted(X_test.columns.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in X_train.columns:\n",
    "    if (X_train[f].dtype=='object') or (X_test[f].dtype=='object'): \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(X_train[f])+list(X_test[f]))\n",
    "        X_train[f] = lbl.transform(list(X_train[f]))\n",
    "        X_test[f] = lbl.transform(list(X_test[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug for \"LightGBMError: Do not support special JSON characters in feature name.\"\n",
    "\n",
    "X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "X_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17690, 1206)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random1(ins_id,sample_size):\n",
    "    list1 = []\n",
    "    for id1 in ins_id.unique():\n",
    "        temp_list = list(ins_id[ins_id == id1].index)\n",
    "        list1.append(list(np.random.choice(temp_list,sample_size)))\n",
    "    return np.array(list1)\n",
    "\n",
    "truncated_groups = random1(ins_id,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th seed begins----------------------\n",
      "mean rse: 0.9773132061309355\n",
      "mean kappa: 0.6072072486834931\n",
      "oof kappa: 0.6077892885875736\n",
      "[1.12667  1.664135 2.196075]\n",
      "mean cappa 0.5697804230719471\n",
      "New threshold [1.1266700000000003, 1.6641349999999964, 2.196074999999993]\n",
      "3.0    481\n",
      "2.0    234\n",
      "0.0    159\n",
      "1.0    126\n",
      "dtype: int64\n",
      "2 th seed begins----------------------\n",
      "mean rse: 0.9780221665677348\n",
      "mean kappa: 0.6070165383309671\n",
      "oof kappa: 0.6075942027286476\n",
      "[1.12566  1.69287  2.210065]\n",
      "mean cappa 0.5694769938211551\n",
      "New threshold [1.1256600000000008, 1.6928699999999954, 2.210064999999994]\n",
      "3.0    475\n",
      "2.0    227\n",
      "0.0    161\n",
      "1.0    137\n",
      "dtype: int64\n",
      "3 th seed begins----------------------\n",
      "mean rse: 0.9775903398848679\n",
      "mean kappa: 0.6093381757760039\n",
      "oof kappa: 0.6099032043945689\n",
      "[1.12636  1.668895 2.204195]\n",
      "mean cappa 0.5690003386355874\n",
      "New threshold [1.1263600000000002, 1.6688949999999987, 2.204195000000003]\n",
      "3.0    478\n",
      "2.0    237\n",
      "0.0    163\n",
      "1.0    122\n",
      "dtype: int64\n",
      "4 th seed begins----------------------\n",
      "mean rse: 0.9775021355770512\n",
      "mean kappa: 0.6084153747477131\n",
      "oof kappa: 0.608957404321339\n",
      "[1.13366  1.674675 2.22485 ]\n",
      "mean cappa 0.5688853708259589\n",
      "New threshold [1.1336600000000006, 1.6746749999999935, 2.224850000000001]\n",
      "3.0    463\n",
      "2.0    252\n",
      "0.0    161\n",
      "1.0    124\n",
      "dtype: int64\n",
      "5 th seed begins----------------------\n",
      "mean rse: 0.9776022456005651\n",
      "mean kappa: 0.6072904939781947\n",
      "oof kappa: 0.6078666961724171\n",
      "[1.114675 1.657705 2.19882 ]\n",
      "mean cappa 0.5691201692553458\n",
      "New threshold [1.1146750000000019, 1.6577049999999955, 2.19882]\n",
      "3.0    481\n",
      "2.0    239\n",
      "0.0    158\n",
      "1.0    122\n",
      "dtype: int64\n",
      "Overall situation-----------------------\n",
      "Mean rmse 0.97761\n",
      "Mean cappa 0.56925\n",
      "0.49595132736420133\n",
      "0.6321759171187682\n"
     ]
    }
   ],
   "source": [
    "pred_lgb,impor1,oof_class_lgb,oof_num_lgb,pred_num1 = run_all_lgb(X_train,y_train,ins_id,X_test)\n",
    "print(qwk(oof_class_lgb[X_train.num_th_assess ==0],y_train[X_train.num_th_assess ==0]))\n",
    "print(qwk(oof_class_lgb[X_train.num_th_assess !=0],y_train[X_train.num_th_assess !=0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>165255.487808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_ratio</th>\n",
       "      <td>17668.198995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_3_mean_same_assess_grp</th>\n",
       "      <td>16299.823555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accum_avg_group</th>\n",
       "      <td>14452.960512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_game_acc</th>\n",
       "      <td>13501.301559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottle_Filler__Activity__2010</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottle_Filler__Activity__4080</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bubble_Bath_2000</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mushroom_Sorter__Assessment__2025</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bfc77bd6</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1206 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      importance\n",
       "title                              165255.487808\n",
       "0_ratio                             17668.198995\n",
       "last_3_mean_same_assess_grp         16299.823555\n",
       "accum_avg_group                     14452.960512\n",
       "mean_game_acc                       13501.301559\n",
       "...                                          ...\n",
       "Bottle_Filler__Activity__2010           0.000000\n",
       "Bottle_Filler__Activity__4080           0.000000\n",
       "Bubble_Bath_2000                        0.000000\n",
       "Mushroom_Sorter__Assessment__2025       0.000000\n",
       "bfc77bd6                                0.000000\n",
       "\n",
       "[1206 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(impor1,index=X_train.columns,columns=['importance']).sort_values('importance',ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_impor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.475\n",
       "2    0.238\n",
       "0    0.161\n",
       "1    0.126\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission['accuracy_group'] = pred_lgb.astype(int)\n",
    "sample_submission.to_csv('./submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
