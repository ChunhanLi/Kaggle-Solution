{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T18:51:35.501065Z",
     "start_time": "2019-10-03T18:51:33.530311Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold,TimeSeriesSplit,KFold,GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sqlite3\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "import gc\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:10:23.426342Z",
     "start_time": "2019-10-03T17:10:18.811648Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_raw.pkl')\n",
    "test = pd.read_pickle('test_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:10:26.066808Z",
     "start_time": "2019-10-03T17:10:24.788948Z"
    }
   },
   "outputs": [],
   "source": [
    "START_DATE = '2017-11-30'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "train['TransactionDT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "test['TransactionDT'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "for df in [train,test]:\n",
    "    df['DT_D'] = ((df['TransactionDT'].dt.year-2017)*365 + df['TransactionDT'].dt.dayofyear).astype(np.int16)\n",
    "    df['DT_W'] = (df['TransactionDT'].dt.year-2017)*52 + df['TransactionDT'].dt.weekofyear\n",
    "    df['DT_M'] = (df['TransactionDT'].dt.year-2017)*12 + df['TransactionDT'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:10:33.484193Z",
     "start_time": "2019-10-03T17:10:30.389440Z"
    }
   },
   "outputs": [],
   "source": [
    "#### R\n",
    "te = train.groupby(['ProductCD','DT_D'])['isFraud'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "train['ProductCD_R_Day'] = pd.merge(train[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "te = test.groupby(['ProductCD','DT_D'])['TransactionAmt'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "test['ProductCD_R_Day'] = pd.merge(test[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "train.loc[train.ProductCD != 'R','ProductCD_R_Day'] = -999\n",
    "test.loc[test.ProductCD != 'R','ProductCD_R_Day'] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:10:36.085929Z",
     "start_time": "2019-10-03T17:10:35.677278Z"
    }
   },
   "outputs": [],
   "source": [
    "#### H\n",
    "te = train.groupby(['ProductCD','DT_D'])['isFraud'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "train['ProductCD_H_Day'] = pd.merge(train[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "te = test.groupby(['ProductCD','DT_D'])['TransactionAmt'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "test['ProductCD_H_Day'] = pd.merge(test[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "train.loc[train.ProductCD != 'H','ProductCD_H_Day'] = -999\n",
    "test.loc[test.ProductCD != 'H','ProductCD_H_Day'] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:10:41.540469Z",
     "start_time": "2019-10-03T17:10:41.008930Z"
    }
   },
   "outputs": [],
   "source": [
    "#### C\n",
    "te = train.groupby(['ProductCD','DT_D'])['isFraud'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "train['ProductCD_C_Day'] = pd.merge(train[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "te = test.groupby(['ProductCD','DT_D'])['TransactionAmt'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "test['ProductCD_C_Day'] = pd.merge(test[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "train.loc[train.ProductCD != 'C','ProductCD_C_Day'] = 999999\n",
    "test.loc[test.ProductCD != 'C','ProductCD_C_Day'] = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:10:47.053050Z",
     "start_time": "2019-10-03T17:10:46.658053Z"
    }
   },
   "outputs": [],
   "source": [
    "#### W\n",
    "te = train.groupby(['ProductCD','DT_D'])['isFraud'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "train['ProductCD_W_Day'] = pd.merge(train[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "te = test.groupby(['ProductCD','DT_D'])['TransactionAmt'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "test['ProductCD_W_Day'] = pd.merge(test[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "train.loc[train.ProductCD != 'W','ProductCD_W_Day'] = -999\n",
    "test.loc[test.ProductCD != 'W','ProductCD_W_Day'] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:10:52.197888Z",
     "start_time": "2019-10-03T17:10:51.765865Z"
    }
   },
   "outputs": [],
   "source": [
    "#### S\n",
    "te = train.groupby(['ProductCD','DT_D'])['isFraud'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "train['ProductCD_S_Day'] = pd.merge(train[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "te = test.groupby(['ProductCD','DT_D'])['TransactionAmt'].agg(['count','mean'])\n",
    "te.reset_index(inplace=True)\n",
    "test['ProductCD_S_Day'] = pd.merge(test[['ProductCD','DT_D']],te[['ProductCD','DT_D','count']],on = ['ProductCD','DT_D'],how='left')['count']\n",
    "train.loc[train.ProductCD != 'S','ProductCD_S_Day'] = -999\n",
    "test.loc[test.ProductCD != 'S','ProductCD_S_Day'] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:10:57.861897Z",
     "start_time": "2019-10-03T17:10:57.841627Z"
    }
   },
   "outputs": [],
   "source": [
    "train['open_card'] = train.DT_D - train.D1\n",
    "train['first_tran'] = train.DT_D - train.D2\n",
    "test['open_card'] = test.DT_D - test.D1\n",
    "test['first_tran'] = test.DT_D - test.D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:11:09.991437Z",
     "start_time": "2019-10-03T17:11:05.999021Z"
    }
   },
   "outputs": [],
   "source": [
    "train['uid1'] = train.card1.astype(str) +' '+ train.card2.astype(str)+' '+ train.card3.astype(str)+' '+train.card4.astype(str)+' '+ train.card5.astype(str)+' '+ train.card6.astype(str) +' '+ train.addr1.astype(str)+' '+train.addr2.astype(str)+' '+train.open_card.astype(str)\n",
    "test['uid1'] = test.card1.astype(str) +' '+ test.card2.astype(str)+' '+ test.card3.astype(str)+' '+ test.card4.astype(str)+' '+ test.card5.astype(str)+' '+ test.card6.astype(str) +' '+ test.addr1.astype(str)+' '+test.addr2.astype(str)+' '+test.open_card.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.open_card = (train.open_card - train.open_card.min())/(train.open_card.max() - train.open_card.min())\n",
    "test.open_card = (test.open_card - test.open_card.min())/(test.open_card.max() - test.open_card.min())\n",
    "train.first_tran = (train.first_tran - train.first_tran.min())/(train.first_tran.max() - train.first_tran.min())\n",
    "test.first_tran = (test.first_tran - test.first_tran.min())/(test.first_tran.max() - test.first_tran.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:11:33.457323Z",
     "start_time": "2019-10-03T17:11:33.451908Z"
    }
   },
   "outputs": [],
   "source": [
    "def device_hash(x):\n",
    "    s =  str(x['id_30'])+str(x['id_31'])+str(x['id_32'])+str(x['id_33'])+str( x['DeviceType'])+ str(x['DeviceInfo'])\n",
    "    h = hashlib.sha256(s.encode('utf-8')).hexdigest()[0:15]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:13:18.471234Z",
     "start_time": "2019-10-03T17:11:37.938165Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in [train,test]:\n",
    "    df['device_hash'] = df.apply(lambda x: device_hash(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:13:23.016334Z",
     "start_time": "2019-10-03T17:13:19.415737Z"
    }
   },
   "outputs": [],
   "source": [
    "concat_df = pd.concat([train[['uid1','device_hash']],test[['uid1','device_hash']]])\n",
    "tmp = concat_df.groupby('uid1')['device_hash'].agg(['nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:13:47.621461Z",
     "start_time": "2019-10-03T17:13:45.734228Z"
    }
   },
   "outputs": [],
   "source": [
    "train['uid_device_nunique'] = train.uid1.map(tmp.to_dict()['nunique'])\n",
    "test['uid_device_nunique'] = test.uid1.map(tmp.to_dict()['nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:13:55.893263Z",
     "start_time": "2019-10-03T17:13:53.446957Z"
    }
   },
   "outputs": [],
   "source": [
    "#concat_df = pd.concat([train[['uid1','device_hash']],test[['uid1','device_hash']]])\n",
    "tmp = concat_df.groupby('device_hash')['uid1'].agg(['nunique'])\n",
    "\n",
    "train['device_uid_nunique'] = train.device_hash.map(tmp.to_dict()['nunique'])\n",
    "test['device_uid_nunique'] = test.device_hash.map(tmp.to_dict()['nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:19:26.023698Z",
     "start_time": "2019-10-03T17:13:59.525963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 700)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train['uid1'] = train.card1.astype(str) +' '+ train.card2.astype(str)+' '+ train.card3.astype(str)+' '+train.card4.astype(str)+' '+ train.card5.astype(str)+' '+ train.card6.astype(str) +' '+ train.addr1.astype(str)+' '+train.addr2.astype(str)+' '+train.open_card.astype(str)\n",
    "# test['uid1'] = test.card1.astype(str) +' '+ test.card2.astype(str)+' '+ test.card3.astype(str)+' '+ test.card4.astype(str)+' '+ test.card5.astype(str)+' '+ test.card6.astype(str) +' '+ test.addr1.astype(str)+' '+test.addr2.astype(str)+' '+test.open_card.astype(str)\n",
    "\n",
    "###train['uid2'] = train.card1.astype(str) +' '+ train.card2.astype(str)+' '+ train.card3.astype(str)+' '+train.card4.astype(str)+' '+ train.card5.astype(str)+' '+ train.card6.astype(str) +' '+ train.addr1.astype(str)+' '+train.addr2.astype(str)+' '+train.open_card.astype(str)+' '+train.first_tran.astype(str)\n",
    "###test['uid2'] = test.card1.astype(str) +' '+ test.card2.astype(str)+' '+ test.card3.astype(str)+' '+ test.card4.astype(str)+' '+ test.card5.astype(str)+' '+ test.card6.astype(str) +' '+ test.addr1.astype(str)+' '+test.addr2.astype(str)+' '+test.open_card.astype(str)+' '+test.first_tran.astype(str)\n",
    "\n",
    "# train['uid3'] = train.card1.astype(str) +' '+ train.card2.astype(str)+' '+ train.card3.astype(str)+' '+train.card4.astype(str)+' '+ train.card5.astype(str)+' '+ train.card6.astype(str) +' '+ train.addr1.astype(str)+' '+train.addr2.astype(str)+' '+train.open_card.astype(str)+' '+train.first_tran.astype(str)+' '+train.P_emaildomain.astype(str)\n",
    "# test['uid3'] = test.card1.astype(str) +' '+ test.card2.astype(str)+' '+ test.card3.astype(str)+' '+ test.card4.astype(str)+' '+ test.card5.astype(str)+' '+ test.card6.astype(str) +' '+ test.addr1.astype(str)+' '+test.addr2.astype(str)+' '+test.open_card.astype(str)+' '+test.first_tran.astype(str)+' '+test.P_emaildomain.astype(str)\n",
    "\n",
    "def change(hoge):\n",
    "    hoge = np.round(hoge,3)\n",
    "    num = 3\n",
    "    hoge = int(np.round(np.round(hoge,3)*1000))\n",
    "    while(hoge % 10 ==0):\n",
    "        num = num-1\n",
    "        hoge = hoge /10\n",
    "    if num<0:\n",
    "        num = 0\n",
    "    return num\n",
    "  \n",
    "train['decimal_digit'] = train[\"TransactionAmt\"].map(change)\n",
    "test['decimal_digit'] = test['TransactionAmt'].map(change)\n",
    "import gc\n",
    "gc.collect()\n",
    "train.had_id = train.had_id.fillna(0)\n",
    "test.had_id = test.had_id.fillna(0)\n",
    "cat_columns = ['uid1','id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n",
    "            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n",
    "            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9','hour','dow','device_name', 'device_version', 'OS_id_30',  'browser_id_31']\n",
    "count_columns = ['uid1','id_13','id_14','id_17','id_18','id_19','id_20','id_21',\n",
    "                 'id_22','id_24','id_25','id_26','id_30','id_31','id_33',\n",
    "                 'DeviceInfo','card6','P_emaildomain','R_emaildomain','card1',\n",
    "                 'card2','card3','card5','addr1','addr2','hour','device_version','OS_id_30','browser_id_31']\n",
    "### scale\n",
    "for t in ['D15','D2','D1','D4','D6','D10','D11','D12']:\n",
    "    train[t+'_revised'] = train[t]/train.groupby('DT_W')[t].transform('max')\n",
    "    test[t+'_revised'] = test[t]/test.groupby('DT_W')[t].transform('max')\n",
    "for t in ['D3','D5','D7','D8','D13']:\n",
    "    train[t+'_revised'] = train[t]/train.groupby('DT_M')[t].transform('max')\n",
    "    test[t+'_revised'] = test[t]/test.groupby('DT_M')[t].transform('max')\n",
    "train['D14_revised'] = train['D14']/train.groupby('DT_W')['D14'].transform('max')\n",
    "test['D14_revised'] = test['D14']/test.groupby('DT_W')['D14'].transform('max')\n",
    "test.loc[test.DT_W == 78 ,'D14_revised'] = test.loc[test.DT_W == 78 ,'D14_revised'].map(lambda x: np.nan if pd.isna(x) else x/900*530)\n",
    "### \n",
    "train['dow'] = train['TransactionDT'].dt.dayofweek\n",
    "train['hour'] = train['TransactionDT'].dt.hour\n",
    "test['dow'] = test['TransactionDT'].dt.dayofweek\n",
    "test['hour'] = test['TransactionDT'].dt.hour\n",
    "# train['month'] = train['TransactionDT'].dt.month\n",
    "# test['month'] = test['TransactionDT'].dt.month\n",
    "train['email_domain_comp'] = (train['P_emaildomain'].values == train['R_emaildomain'].values).astype(int)\n",
    "test['email_domain_comp'] = (test['P_emaildomain'].values == test['R_emaildomain'].values).astype(int)\n",
    "train.drop(['D9'],axis=1,inplace=True)\n",
    "test.drop(['D9'],axis=1,inplace=True)\n",
    "# X_train = train.drop(['TransactionID','TransactionDT'],axis=1)\n",
    "# X_test = test.drop(['TransactionID','TransactionDT'],axis=1)\n",
    "for f in cat_columns:\n",
    "    #if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train[f].astype(str)) + list(test[f].astype(str)))\n",
    "    train[f] = lbl.transform(list(train[f].astype(str)))\n",
    "    test[f] = lbl.transform(list(test[f].astype(str))) \n",
    "train.fillna(-999,inplace = True)\n",
    "test.fillna(-999,inplace = True)\n",
    "for i in count_columns:\n",
    "    train[i+'_count_full'] = train[i].map(pd.concat([train[i], test[i]], ignore_index=True).value_counts(dropna=False))\n",
    "    test[i+'_count_full'] = test[i].map(pd.concat([train[i], test[i]], ignore_index=True).value_counts(dropna=False))\n",
    "#train['decimal_digit_count_full'] = train['decimal_digit'].map(pd.concat([train['decimal_digit'], test['decimal_digit']], ignore_index=True).value_counts(dropna=False))\n",
    "#test['decimal_digit_count_full'] = test['decimal_digit'].map(pd.concat([train['decimal_digit'], test['decimal_digit']], ignore_index=True).value_counts(dropna=False))\n",
    "train_test_all = pd.concat([train,test],ignore_index=True,sort=False)\n",
    "train_test_all['day_count'] = train_test_all.groupby(train_test_all.TransactionDT.dt.date)['TransactionAmt'].transform('count')\n",
    "train_test_all['hour_count'] = train_test_all.groupby(train_test_all.TransactionDT.map(lambda x:str(x)[:13]))['TransactionAmt'].transform('count')\n",
    "train['day_count'] = train_test_all[:590540].day_count.tolist()\n",
    "test['day_count'] = train_test_all[590540:].day_count.tolist()\n",
    "train['hour_count'] = train_test_all[:590540].hour_count.tolist()\n",
    "test['hour_count'] = train_test_all[590540:].hour_count.tolist()\n",
    "# a= mean_encode(train,test,columns=cat_columns,target_col='isFraud',\n",
    "#                reg_method = 'k_fold',folds=5,alpha =5 )\n",
    "# for i in ['id_15', 'id_16', 'M4', 'card5','DeviceInfo']:\n",
    "#     train['mean_'+i] = a['mean_isFraud_'+i][:590540].tolist()\n",
    "#     test['mean_'+i] = a['mean_isFraud_'+i][590540:].tolist()\n",
    "y_train = train['isFraud'].copy()\n",
    "X_train = train.drop(['TransactionID','isFraud','TransactionDT'],axis=1)\n",
    "X_test = test.drop(['TransactionID','TransactionDT'],axis=1)\n",
    "del train_test_all\n",
    "### add new\n",
    "temp123 = ['TransactionAmt__ProductCD']\n",
    "for feature in temp123:\n",
    "    f1, f2 = feature.split('__')\n",
    "    X_train[feature] = X_train[f1].astype(str) + '_' + X_train[f2].astype(str)\n",
    "    X_test[feature] = X_test[f1].astype(str) + '_' + X_test[f2].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(X_train[feature].astype(str).values) + list(X_test[feature].astype(str).values))\n",
    "    X_train[feature] = le.transform(list(X_train[feature].astype(str).values))\n",
    "    X_test[feature] = le.transform(list(X_test[feature].astype(str).values))\n",
    "X_train.rename(columns = {'TransactionAmt__ProductCD':'ProductID'},inplace=True)\n",
    "X_test.rename(columns = {'TransactionAmt__ProductCD':'ProductID'},inplace=True)\n",
    "for i in ['ProductID']:\n",
    "    X_train[i+'_count_full'] = X_train[i].map(pd.concat([X_train[i], X_test[i]], ignore_index=True).value_counts(dropna=False))\n",
    "    X_test[i+'_count_full'] = X_test[i].map(pd.concat([X_train[i], X_test[i]], ignore_index=True).value_counts(dropna=False))\n",
    "###    \n",
    "temp = ['DeviceInfo__P_emaildomain', \n",
    "        'card1__card5', \n",
    "        'card2__id_20',\n",
    "        'card5__P_emaildomain', \n",
    "        'addr1__card1',\n",
    "        'addr1__addr2',\n",
    "        'card1__card2',\n",
    "        'card2__addr1',\n",
    "        'card1__P_emaildomain',\n",
    "        'card2__P_emaildomain',\n",
    "        'addr1__P_emaildomain',\n",
    "        'DeviceInfo__id_31',\n",
    "        'DeviceInfo__id_20',\n",
    "        'DeviceType__id_31',\n",
    "        'DeviceType__id_20',\n",
    "        'DeviceType__P_emaildomain',\n",
    "        'card1__M4',\n",
    "        'card2__M4',\n",
    "        'addr1__M4',\n",
    "        'P_emaildomain__M4',\n",
    "       'uid1__ProductID',\n",
    "       'uid1__DeviceInfo']\n",
    "for feature in temp:\n",
    "    f1, f2 = feature.split('__')\n",
    "    X_train[feature] = X_train[f1].astype(str) + '_' + X_train[f2].astype(str)\n",
    "    X_test[feature] = X_test[f1].astype(str) + '_' + X_test[f2].astype(str)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(X_train[feature].astype(str).values) + list(X_test[feature].astype(str).values))\n",
    "    X_train[feature] = le.transform(list(X_train[feature].astype(str).values))\n",
    "    X_test[feature] = le.transform(list(X_test[feature].astype(str).values))\n",
    "\n",
    "for i in temp:\n",
    "    X_train[i+'_count_full'] = X_train[i].map(pd.concat([X_train[i], X_test[i]], ignore_index=True).value_counts(dropna=False))\n",
    "    X_test[i+'_count_full'] = X_test[i].map(pd.concat([X_train[i], X_test[i]], ignore_index=True).value_counts(dropna=False))\n",
    "con_fea = ['V258','C1','C14','C13','TransactionAmt','D15_revised','D2_revised','id_02','dist1','V294','C11']\n",
    "cat_fea = ['card1','card2','addr1','card4','R_emaildomain','P_emaildomain','ProductID','uid1']\n",
    "train_test = pd.concat([X_train,X_test],ignore_index=True,sort=False)\n",
    "for cont in con_fea:\n",
    "    for cat in cat_fea:\n",
    "        X_train[cont+'_'+cat+'_mean'] = train_test[cont].map(lambda x:np.nan if x==-999 else x).groupby(train_test[cat]).transform('mean')[:590540].tolist()\n",
    "        X_train[cont+'_'+cat+'_std'] = train_test[cont].map(lambda x:np.nan if x==-999 else x).groupby(train_test[cat]).transform('std')[:590540].tolist()\n",
    "        X_test[cont+'_'+cat+'_mean'] = train_test[cont].map(lambda x:np.nan if x==-999 else x).groupby(train_test[cat]).transform('mean')[590540:].tolist()\n",
    "        X_test[cont+'_'+cat+'_std'] =  train_test[cont].map(lambda x:np.nan if x==-999 else x).groupby(train_test[cat]).transform('std')[590540:].tolist()\n",
    "X_train.fillna(-999,inplace=True)\n",
    "X_test.fillna(-999,inplace=True)\n",
    "X_train.drop(['DeviceInfo','device_version','DT_D','DT_W','DT_M','D15',\n",
    "              'D2','D1','D4','D6','D10','D11','D12','D3','D5','D7','D8','D13','D14','TransactionAmt_ProductID_mean'],axis=1,inplace=True)\n",
    "X_test.drop(['DeviceInfo','device_version','DT_D','DT_W','DT_M','D15',\n",
    "             'D2','D1','D4','D6','D10','D11','D12','D3','D5','D7','D8','D13','D14','TransactionAmt_ProductID_mean'],axis=1,inplace=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:19:28.497945Z",
     "start_time": "2019-10-03T17:19:26.874791Z"
    }
   },
   "outputs": [],
   "source": [
    "###除掉之前350之后的特征\n",
    "orders = pd.read_csv('importance.csv')\n",
    "drop = orders.loc[350:,'Unnamed: 0'].tolist()\n",
    "X_train.drop(drop,axis=1,inplace=True)\n",
    "X_test.drop(drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:19:29.358369Z",
     "start_time": "2019-10-03T17:19:29.355174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 359)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:19:30.170639Z",
     "start_time": "2019-10-03T17:19:30.153115Z"
    }
   },
   "outputs": [],
   "source": [
    "cat = ['uid1','id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n",
    "            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n",
    "            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9','hour','dow','device_name', 'OS_id_30',  'browser_id_31','ProductID',\n",
    "'DeviceInfo__P_emaildomain', \n",
    "        'card1__card5', \n",
    "        'card2__id_20',\n",
    "        'card5__P_emaildomain', \n",
    "        'addr1__card1',\n",
    "        'addr1__addr2',\n",
    "        'card1__card2',\n",
    "        'card2__addr1',\n",
    "        'card1__P_emaildomain',\n",
    "        'card2__P_emaildomain',\n",
    "        'addr1__P_emaildomain',\n",
    "        'DeviceInfo__id_31',\n",
    "        'DeviceInfo__id_20',\n",
    "        'DeviceType__id_31',\n",
    "        'DeviceType__id_20',\n",
    "        'DeviceType__P_emaildomain',\n",
    "        'card1__M4',\n",
    "        'card2__M4',\n",
    "        'addr1__M4',\n",
    "        'P_emaildomain__M4',\n",
    "       'uid1__ProductID',\n",
    "       'uid1__DeviceInfo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:19:30.998858Z",
     "start_time": "2019-10-03T17:19:30.996739Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in drop:\n",
    "    if i in cat:\n",
    "        cat.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:19:52.370648Z",
     "start_time": "2019-10-03T17:19:31.656771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "uid1\n",
      "train: 0.12\n",
      "test: 0.14\n",
      "----------------------------------------\n",
      "id_13\n",
      "train: 0.49\n",
      "test: 0.96\n",
      "----------------------------------------\n",
      "id_14\n",
      "train: 0.96\n",
      "test: 0.89\n",
      "----------------------------------------\n",
      "id_18\n",
      "train: 0.89\n",
      "test: 0.94\n",
      "----------------------------------------\n",
      "id_19\n",
      "train: 0.87\n",
      "test: 0.91\n",
      "----------------------------------------\n",
      "id_20\n",
      "train: 0.72\n",
      "test: 0.65\n",
      "----------------------------------------\n",
      "id_30\n",
      "train: 0.99\n",
      "test: 0.86\n",
      "----------------------------------------\n",
      "id_31\n",
      "train: 0.72\n",
      "test: 0.69\n",
      "----------------------------------------\n",
      "id_33\n",
      "train: 0.73\n",
      "test: 0.49\n",
      "----------------------------------------\n",
      "card6\n",
      "train: 0.80\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "M4\n",
      "train: 1.00\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "P_emaildomain\n",
      "train: 1.00\n",
      "test: 0.98\n",
      "----------------------------------------\n",
      "R_emaildomain\n",
      "train: 1.00\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "card1\n",
      "train: 0.72\n",
      "test: 0.73\n",
      "----------------------------------------\n",
      "card2\n",
      "train: 0.99\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "card5\n",
      "train: 0.70\n",
      "test: 0.82\n",
      "----------------------------------------\n",
      "addr1\n",
      "train: 0.55\n",
      "test: 0.63\n",
      "----------------------------------------\n",
      "M2\n",
      "train: 1.00\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "M3\n",
      "train: 1.00\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "M5\n",
      "train: 1.00\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "M6\n",
      "train: 1.00\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "M7\n",
      "train: 1.00\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "hour\n",
      "train: 1.00\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "dow\n",
      "train: 1.00\n",
      "test: 1.00\n",
      "----------------------------------------\n",
      "device_name\n",
      "train: 0.83\n",
      "test: 0.88\n",
      "----------------------------------------\n",
      "ProductID\n",
      "train: 0.25\n",
      "test: 0.37\n",
      "----------------------------------------\n",
      "DeviceInfo__P_emaildomain\n",
      "train: 0.40\n",
      "test: 0.37\n",
      "----------------------------------------\n",
      "card1__card5\n",
      "train: 0.69\n",
      "test: 0.70\n",
      "----------------------------------------\n",
      "card2__id_20\n",
      "train: 0.67\n",
      "test: 0.67\n",
      "----------------------------------------\n",
      "card5__P_emaildomain\n",
      "train: 0.75\n",
      "test: 0.82\n",
      "----------------------------------------\n",
      "addr1__card1\n",
      "train: 0.56\n",
      "test: 0.59\n",
      "----------------------------------------\n",
      "addr1__addr2\n",
      "train: 0.51\n",
      "test: 0.57\n",
      "----------------------------------------\n",
      "card1__card2\n",
      "train: 0.69\n",
      "test: 0.70\n",
      "----------------------------------------\n",
      "card2__addr1\n",
      "train: 0.72\n",
      "test: 0.76\n",
      "----------------------------------------\n",
      "card1__P_emaildomain\n",
      "train: 0.54\n",
      "test: 0.59\n",
      "----------------------------------------\n",
      "card2__P_emaildomain\n",
      "train: 0.76\n",
      "test: 0.82\n",
      "----------------------------------------\n",
      "addr1__P_emaildomain\n",
      "train: 0.75\n",
      "test: 0.78\n",
      "----------------------------------------\n",
      "DeviceInfo__id_31\n",
      "train: 0.13\n",
      "test: 0.14\n",
      "----------------------------------------\n",
      "DeviceInfo__id_20\n",
      "train: 0.32\n",
      "test: 0.26\n",
      "----------------------------------------\n",
      "DeviceType__id_31\n",
      "train: 0.73\n",
      "test: 0.69\n",
      "----------------------------------------\n",
      "DeviceType__id_20\n",
      "train: 0.72\n",
      "test: 0.64\n",
      "----------------------------------------\n",
      "DeviceType__P_emaildomain\n",
      "train: 0.99\n",
      "test: 0.99\n",
      "----------------------------------------\n",
      "card1__M4\n",
      "train: 0.64\n",
      "test: 0.68\n",
      "----------------------------------------\n",
      "card2__M4\n",
      "train: 0.92\n",
      "test: 0.95\n",
      "----------------------------------------\n",
      "addr1__M4\n",
      "train: 0.64\n",
      "test: 0.70\n",
      "----------------------------------------\n",
      "P_emaildomain__M4\n",
      "train: 0.94\n",
      "test: 0.97\n",
      "----------------------------------------\n",
      "uid1__ProductID\n",
      "train: 0.04\n",
      "test: 0.05\n",
      "----------------------------------------\n",
      "uid1__DeviceInfo\n",
      "train: 0.10\n",
      "test: 0.12\n"
     ]
    }
   ],
   "source": [
    "for column in cat:\n",
    "    train_set = set(X_train[column])\n",
    "    test_set = set(X_test[column])\n",
    "    tt = train_set.intersection(test_set)\n",
    "    print('----------------------------------------')\n",
    "    print(column)\n",
    "    print('train:','{:.2f}'.format(len(tt)/len(train_set)))\n",
    "    print('test:','{:.2f}'.format(len(tt)/len(test_set)))\n",
    "    X_train[column] = X_train[column].map(lambda x: -999 if x not in tt else x)\n",
    "    X_test[column] = X_test[column].map(lambda x: -999 if x not in tt else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T17:19:54.798105Z",
     "start_time": "2019-10-03T17:19:53.275429Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.drop(['device_hash'],axis=1,inplace=True)\n",
    "X_test.drop(['device_hash'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits = 5)\n",
    "resu1 = 0\n",
    "impor1 = 0\n",
    "y_pred = 0\n",
    "stack_train = np.zeros([X_train.shape[0],])\n",
    "for train_index, test_index in kf.split(X_train, y_train):\n",
    "    X_train2= X_train.iloc[train_index,:]\n",
    "    y_train2= y_train.iloc[train_index]\n",
    "    X_test2= X_train.iloc[test_index,:]\n",
    "    y_test2= y_train.iloc[test_index]\n",
    "    clf = lgb.LGBMClassifier(n_estimators=10000, random_state=47,subsample=0.7,\n",
    "                             colsample_bytree=0.7,learning_rate=0.005,importance_type = 'gain',\n",
    "                     max_depth = -1, num_leaves = 256,min_child_samples=20,min_split_gain = 0.001,\n",
    "                       bagging_freq=1,reg_alpha = 0,reg_lambda = 0,n_jobs = -1,metric='None')\n",
    "    clf.fit(X_train2,y_train2,eval_set = [(X_train2,y_train2),(X_test2,y_test2)], eval_metric = 'auc',early_stopping_rounds=500,verbose=100)\n",
    "    temp_predict = clf.predict_proba(X_test2)[:,1]\n",
    "    stack_train[test_index] = temp_predict\n",
    "    y_pred += clf.predict_proba(X_test)[:,1]/5\n",
    "    roc = roc_auc_score(y_test2, temp_predict)\n",
    "    print(roc)\n",
    "    resu1 += roc/5\n",
    "    impor1 += clf.feature_importances_/5\n",
    "    gc.collect()\n",
    "print('End:',resu1)\n",
    "resu = pd.read_csv('../../sample_submission.csv')\n",
    "resu['isFraud'] = y_pred\n",
    "resu.to_csv('10_4_new.csv',index=False)\n",
    "a= pd.DataFrame()\n",
    "a['train'] = stack_train\n",
    "a.to_csv('10_4_new.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
